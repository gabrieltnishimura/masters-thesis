\chapter*[Introduction]{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

Natural language processing is a vast field that explores how computers can understand and manipulate human language in text or speech format. Researches in this area includes (but are not limited to) sentiment analysis, sentence prediction, text translation, text to speech conversion and voice recognition. Voice recognition in particular is a class of machine learning that can be stochastically modelled - using Hidden Markov Models \cite{gales2008application} -, Neural Networks \cite{graves2013speech} or even non-stochastically \cite{burget2003nonrandomattr}.

However, the suitability of these solutions degrades according to the quality and quantity of data used in training stages. Factors such as noisy speech data, non-homogeneous recordings, different microphones within the same dataset and even speech disorders could limit proper analysis, affect accuracy and even change speech predictions. 

There are many robust datasets available in the literature, such as the TIMIT \cite{Lamel1992timmit}, DIRHA \cite{Ravanelli2016dirha} and the more recent \cite{chanchaochai2018globaltimit}. These datasets are called Speech Corpora and have a collection of audio recordings of spoken language. Some of them also have additional text files containing transcriptions of the words spoken. Unfortunately, most speech corpora are for the English language \cite{LeRouxVincent2014TRdatasets} and the literature on the Speech Corpora creation itself is sparse. 

In order to fill this gap, this work presents a systematic literature review of speech corpora to define its characteristics and requirements when creating this kind of dataset.

\chapter{Objective}

The main focus of this article lies in defining what characterizes a good and robust speech dataset.

The secondary focus of this work applies what was defined as a robust dataset to generate one in brazilian portuguese, uploading it to a public repository. 
